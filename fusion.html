<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Image Fusion</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/yeti.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>





<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Digital Image Processing in R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fas fa-home"></span>
     
    Home
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fas fa-gear"></span>
     
    More
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="lesson1.html">Data storage schema</a>
    </li>
    <li>
      <a href="fusion.html">Image Fusion</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="mailto:bensonkemboi@gmail.com">
    <span class="fas fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="http://github.com/bensonkenduiywo">
    <span class="fab fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/bensonkenduiywo">
    <span class="fab fa-twitter fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/benson-kenduiywo-1a218137">
    <span class="fab fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Image Fusion</h1>

</div>


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>This tutorial seeks to illustrate how image fusion can be conducted. We use an Unmanned Aerial Vehicle (UAV) and Sentinel 2 optical image acquired within the same area and period. <strong>Data fusion</strong> is a formal framework in which are expressed means and tools for the alliance of data originating from different sources. It aims at obtaining information of greater quality; the exact definition of “greater quality” will depend upon the application (<a href="https://doi.org/10.1007/978-1-4020-4385-7_11">Ranchin and Wald, 2010</a>).</p>
<p>The principal motivation for image fusion is to improve the quality of the information contained in the output image in a process known as synergy. A study of existing image fusion techniques and applications shows that image fusion can provide us with an output image with an improved quality. In this case, the benefits of image fusion include:</p>
<ol style="list-style-type: decimal">
<li>Extended range of operation.</li>
<li>Extended spatial and temporal coverage.</li>
<li>Reduced uncertainty.</li>
<li>Increased reliability.</li>
<li>Robust system performance.</li>
<li>Compact representation of information.</li>
</ol>
</div>
<div id="data-preparation" class="section level2">
<h2>Data preparation</h2>
<p>Load libraries, declare variables and data paths.</p>
<pre class="r"><code>rm(list=ls(all=TRUE))    #Clears R memory
unlink(&quot;.RData&quot;) 
if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;); library(pacman) #package manager( loads required packages/libraries in list as below if not installed they will be installed
p_load(raster, terra)

options(warn=1)
cat(&quot;Set variables and start processing\n&quot;)</code></pre>
<pre><code>## Set variables and start processing</code></pre>
<pre class="r"><code>Root        &lt;- &#39;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/&#39;
Path_out    &lt;- paste0(Root,&quot;Output/&quot;)</code></pre>
<p>Load UAV and Sentinel 2 optical images.</p>
<pre class="r"><code>path &lt;- list.files(paste0(Root,&#39;S2/interim/&#39;),pattern = (&quot;.tif$&quot;), recursive = TRUE, full.names = TRUE)
path</code></pre>
<pre><code>## [1] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/S2/interim/RT_T36JUR_20210418T073611_B02.tif&quot;
## [2] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/S2/interim/RT_T36JUR_20210418T073611_B03.tif&quot;
## [3] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/S2/interim/RT_T36JUR_20210418T073611_B04.tif&quot;
## [4] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/S2/interim/RT_T36JUR_20210418T073611_B08.tif&quot;</code></pre>
<pre class="r"><code>s &lt;- rast(path)
s</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 10980, 10980, 4  (nrow, ncol, nlyr)
## resolution  : 10, 10  (x, y)
## extent      : 3e+05, 409800, 6990220, 7100020  (xmin, xmax, ymin, ymax)
## coord. ref. : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs 
## sources     : RT_T36JUR_20210418T073611_B02.tif  
##               RT_T36JUR_20210418T073611_B03.tif  
##               RT_T36JUR_20210418T073611_B04.tif  
##               ... and 1 more source(s)
## names       : RT_T36J~611_B02, RT_T36J~611_B03, RT_T36J~611_B04, RT_T36J~611_B08</code></pre>
<pre class="r"><code>path &lt;- list.files(paste0(Root,&#39;WingtraOne/&#39;),pattern = (&quot;.tif$&quot;), recursive = TRUE, full.names = TRUE)
#reorder the bands to macth those in S2
paths &lt;- path
paths[3] &lt;- path[4]
paths[4] &lt;- path[3]
paths</code></pre>
<pre><code>## [1] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/WingtraOne/mpolonjeni_05042021m3f1_transparent_reflectance_blue.tif&quot; 
## [2] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/WingtraOne/mpolonjeni_05042021m3f1_transparent_reflectance_green.tif&quot;
## [3] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/WingtraOne/mpolonjeni_05042021m3f1_transparent_reflectance_red.tif&quot;  
## [4] &quot;D:/JKUAT/RESEARCH_Projects/Eswatini/Data/WingtraOne/mpolonjeni_05042021m3f1_transparent_reflectance_nir.tif&quot;</code></pre>
<pre class="r"><code>v &lt;- rast(paths)
v</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 12981, 6363, 4  (nrow, ncol, nlyr)
## resolution  : 0.12059, 0.12059  (x, y)
## extent      : 390445, 391212.3, 7070171, 7071736  (xmin, xmax, ymin, ymax)
## coord. ref. : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs 
## sources     : mpolonjeni_05042021m3f1_transparent_reflectance_blue.tif  
##               mpolonjeni_05042021m3f1_transparent_reflectance_green.tif  
##               mpolonjeni_05042021m3f1_transparent_reflectance_red.tif  
##               ... and 1 more source(s)
## names       : mpolonj~ce_blue, mpolonj~e_green, mpolonj~nce_red, mpolonj~nce_nir</code></pre>
<p>Now check image properties and assign meaningful names to its bands.</p>
<pre class="r"><code>#Resolution
res(s)</code></pre>
<pre><code>## [1] 10 10</code></pre>
<pre class="r"><code>#Extents
ext(s)</code></pre>
<pre><code>## SpatExtent : 3e+05, 409800, 6990220, 7100020 (xmin, xmax, ymin, ymax)</code></pre>
<pre class="r"><code>#image dimensions
dim(s)</code></pre>
<pre><code>## [1] 10980 10980     4</code></pre>
<pre class="r"><code>#Number of bands
nlyr(s)</code></pre>
<pre><code>## [1] 4</code></pre>
<pre class="r"><code>names(s) &lt;- c(&quot;b&quot;, &quot;g&quot;,&quot;r&quot;, &quot;nir&quot;)
s</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 10980, 10980, 4  (nrow, ncol, nlyr)
## resolution  : 10, 10  (x, y)
## extent      : 3e+05, 409800, 6990220, 7100020  (xmin, xmax, ymin, ymax)
## coord. ref. : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs 
## sources     : RT_T36JUR_20210418T073611_B02.tif  
##               RT_T36JUR_20210418T073611_B03.tif  
##               RT_T36JUR_20210418T073611_B04.tif  
##               ... and 1 more source(s)
## names       : b, g, r, nir</code></pre>
<pre class="r"><code>res(v)</code></pre>
<pre><code>## [1] 0.12059 0.12059</code></pre>
<pre class="r"><code>ext(v)</code></pre>
<pre><code>## SpatExtent : 390445.0352, 391212.34937, 7070171.12052, 7071736.49931 (xmin, xmax, ymin, ymax)</code></pre>
<pre class="r"><code>dim(v)</code></pre>
<pre><code>## [1] 12981  6363     4</code></pre>
<pre class="r"><code>names(v) &lt;- c(&quot;b&quot;, &quot;g&quot;,&quot;r&quot;, &quot;nir&quot;)
v</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 12981, 6363, 4  (nrow, ncol, nlyr)
## resolution  : 0.12059, 0.12059  (x, y)
## extent      : 390445, 391212.3, 7070171, 7071736  (xmin, xmax, ymin, ymax)
## coord. ref. : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs 
## sources     : mpolonjeni_05042021m3f1_transparent_reflectance_blue.tif  
##               mpolonjeni_05042021m3f1_transparent_reflectance_green.tif  
##               mpolonjeni_05042021m3f1_transparent_reflectance_red.tif  
##               ... and 1 more source(s)
## names       : b, g, r, nir</code></pre>
<p>Crop/clip Sentinel 2 image to UAV image extents.</p>
<pre class="r"><code>s &lt;- crop(s, ext(v), snap=&quot;near&quot;)</code></pre>
<p>Display the images side by side.</p>
<pre class="r"><code>x11()
par(mfrow = c(1, 2)) #c(bottom, left, top, right)
plotRGB(s, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, axes=T, mar = c(4, 5, 1.4, 0.2), main=&quot;S2&quot;, cex.axis=0.5)
box()
plotRGB(v, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, axes=T, mar = c(4, 5, 1.4, 0.2), main=&quot;UAV&quot;, cex.axis=0.5)
box()</code></pre>
<p><img src="fusion_files/figure-html/d5-1.png" width="672" /></p>
<p>First let us conduct a spectral fusion of S2 and UAV images. To do this we have to resample UAV image to S2 extents and another to one to 1 m.</p>
<pre class="r"><code>v_r &lt;- resample(v, s, method=&#39;bilinear&#39;)
ext(v_r) == ext(s)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>temp &lt;-rast(nrow=1570,ncol=760,ext(s)) #empty object to upscale UAV to 1 m resolution
res(temp) </code></pre>
<pre><code>## [1] 1 1</code></pre>
<pre class="r"><code>v1 &lt;- resample(v, temp, method=&#39;bilinear&#39;)
res(v1) </code></pre>
<pre><code>## [1] 1 1</code></pre>
</div>
<div id="pixel-fusion" class="section level2">
<h2>Pixel fusion</h2>
<p>This sections considers fusion techniques which rely on simple pixel based operations on input image values. The assumption is that the input images are spatially and temporally aligned, semantically equivalent and radiometrically calibrated. Therefore, let us fuse the two images by multiplication and display it against the original ones.</p>
<pre class="r"><code>#Fuse by multiplication
f1 &lt;- s * v_r
f1</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 157, 76, 4  (nrow, ncol, nlyr)
## resolution  : 10, 10  (x, y)
## extent      : 390450, 391210, 7070170, 7071740  (xmin, xmax, ymin, ymax)
## coord. ref. : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs 
## source      : memory 
## names       :            b,            g,            r,          nir 
## min values  : 0.0002068561, 0.0005442272, 0.0003042399, 0.0094109712 
## max values  :   0.01279990,   0.01698387,   0.02435488,   0.18333833</code></pre>
<pre class="r"><code>#Display fused image alongside original UAV
x11()
par(mfrow = c(1, 3),mar = c(4, 5, 1.4, 0.2)) 
plotRGB(v, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, axes=T, mar = c(4, 5, 1.4, 0.2), main=&quot;UAV&quot;, cex.axis=0.7)
box()
plotRGB(s, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, axes=T, mar = c(4, 5, 1.4, 0.2), main=&quot;S2&quot;, cex.axis=0.7)
box()
plotRGB(f1, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, axes=T, mar = c(4, 5, 1.4, 0.2), main=&quot;Fused_multi&quot;, cex.axis=0.7)
box()</code></pre>
<p><img src="fusion_files/figure-html/d7-1.png" width="672" /></p>
<p>What about mean fusion (i.e. taking the mean of each pixel’s reflectance in both UAV and S2)?</p>
<pre class="r"><code>#Fuse by multiplication
f2 &lt;- mean(s, v_r)
f2</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 157, 76, 4  (nrow, ncol, nlyr)
## resolution  : 10, 10  (x, y)
## extent      : 390450, 391210, 7070170, 7071740  (xmin, xmax, ymin, ymax)
## coord. ref. : +proj=utm +zone=36 +south +datum=WGS84 +units=m +no_defs 
## source      : memory 
## names       :          b,          g,          r,        nir 
## min values  : 0.01532078, 0.02351120, 0.01861056, 0.09712517 
## max values  :  0.1225800,  0.1548688,  0.1717744,  0.4470450</code></pre>
<pre class="r"><code>#Display fused image alongside original UAV
x11()
par(mfrow = c(2, 2), mar = c(4, 5, 1.4, 0.2)) 
plotRGB(v, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;,  main=&quot;UAV&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(s, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;S2&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f1, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;Fused_mult&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f2, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;Fused_mean&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()</code></pre>
<p><img src="fusion_files/figure-html/d8-1.png" width="672" /></p>
<p>Let us finally follow the fusion approach in <a href="https://ieeexplore.ieee.org/document/8812312">Zou et al (2018)</a>.</p>
<pre class="r"><code>f3 = (s/v_r)*v_r
x11()
par(mfrow = c(2, 3), mar = c(4, 5, 1.4, 0.2)) 
plotRGB(v, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;,  main=&quot;UAV&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(s, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;S2&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f1, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;Fused_mult&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f2, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;Fused_mean&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f3, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;Zhou etal&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()</code></pre>
<p><img src="fusion_files/figure-html/d9-1.png" width="672" /></p>
<p>There seem to be some linear relationship between UAV and Sentinel 2 surface reflectance. However it is evident that reflectance values from UAV are higher compared to those in Sentinel 2. So what now?</p>
</div>
<div id="feature-based-fusion" class="section level2">
<h2>Feature based fusion</h2>
<p>In <em>feature fusion</em> we fuse together the features <span class="math inline">\(F_k,k \in{1,2, \dots, K}\)</span>. These features can be vegetation indices like Normalized Difference Index (NDVI) or feature maps that have been made semantically equivalent by transforming them into probabilistic <span class="math inline">\(p(m,n)\)</span>, or likelihood, maps.</p>
<p>Let us start with NDVI (<span class="math inline">\(\text{NDVI}=\frac{\text{NIR}-\text{Red}}{\text{NIR}+\text{Red}}\)</span>. First compute NDVI for both UAV and S2.</p>
<pre class="r"><code>n_v &lt;- (subset(v_r,&quot;nir&quot;)-subset(v_r,&quot;r&quot;))/(subset(v_r,&quot;nir&quot;)+subset(v_r,&quot;r&quot;))
n_s &lt;- (subset(s,&quot;nir&quot;)-subset(s,&quot;r&quot;))/(subset(s,&quot;nir&quot;)+subset(s,&quot;r&quot;))</code></pre>
<p>How can we fuse the NDVI index? Let us take an average of the two.</p>
<pre class="r"><code>nf &lt;- mean(n_v, n_s)
x11()
par(mfrow = c(2, 2), mar = c(5, 5, 1.4, 0.2))#c(bottom, left, top, right)
plot(n_v, main=&quot;UAV NDVI&quot;)
plot(n_s, main=&quot;S2 NDVI&quot;)
plot(nf, main=&quot;Fused NDVI&quot;)</code></pre>
<p><img src="fusion_files/figure-html/n2-1.png" width="672" /></p>
<p>Is there any difference between S2, UAV, and the fused NDVI images as shown above?</p>
</div>
<div id="spatial-spectral-fusion" class="section level2">
<h2>Spatial-spectral fusion</h2>
<p>Previously we upsampled the UAV image in order to conduct fusion. While this reduces spectral variability it destroys spatial resolution. Therefore, in this section we will first donwsample the Satellite image to match UAV spatial resolution and then proceed to conduct image fusion. This way, we will improve both spatial and spectral information of Sentinel 2 image and spectral information for UAV.</p>
<div id="modelling-reflectance" class="section level3">
<h3>Modelling reflectance</h3>
<ol style="list-style-type: decimal">
<li>Can we improve the resolution of S2 using UAV?</li>
<li>Can we predict UAV reflectance in places not imaged by the drones?</li>
</ol>
<p>Let’s check the relationship between the two.</p>
<pre class="r"><code>x11()
par(mfrow = c(2, 2), mar = c(4, 5, 1.4, 0.2)) 
plot(as.vector(subset(s,&#39;b&#39;)),as.vector(subset(v_r,&#39;b&#39;)), xlab=&#39;S2&#39;, ylab=&#39;W1 UAV&#39;, main=&quot;Blue band&quot;,pch=16,cex=0.75, col=&#39;blue&#39;)
plot(as.vector(subset(s,&#39;g&#39;)),as.vector(subset(v_r,&#39;g&#39;)), xlab=&#39;S2&#39;, ylab=&#39;W1 UAV&#39;, main=&quot;Green band&quot;,pch=16,cex=0.75, col=&#39;green&#39;)
plot(as.vector(subset(s,&#39;r&#39;)),as.vector(subset(v_r,&#39;r&#39;)), xlab=&#39;S2&#39;, ylab=&#39;W1 UAV&#39;, main=&quot;Red band&quot;,pch=16,cex=0.75, col=&#39;red&#39;)
plot(as.vector(subset(s,&#39;nir&#39;)),as.vector(subset(v_r,&#39;nir&#39;)), xlab=&#39;S2&#39;, ylab=&#39;W1 UAV&#39;, main=&quot;NIR band&quot;,pch=16,cex=0.75)</code></pre>
<p><img src="fusion_files/figure-html/m1-1.png" width="672" /></p>
<p>Lets create sample points from S2 (10 m resolution) and upscaled UAV (10 m resolution) and use them to create a model that we can predict S2 reflectance based on UAV 1 m resolution. Essentially what we are doing here is to create a model that can predict S2 reflectance at a resolution of 1 m.</p>
<pre class="r"><code>set.seed(530)
x11()
points &lt;- spatSample(v_r, 900, &quot;random&quot;, as.points=T, na.rm=T, values=F)
plotRGB(v_r, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;,  main=&quot;UAV+sampling points&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
plot(points,add=T)</code></pre>
<p><img src="fusion_files/figure-html/m2-1.png" width="672" /></p>
<p>Now extract reflectance values from the two images.</p>
<pre class="r"><code>s2_p &lt;- extract(s, points, drop=F)
head(s2_p)</code></pre>
<pre><code>##   ID          b          g          r    nir
## 1  1 0.03010000 0.04340000 0.04370000 0.2039
## 2  2 0.02669999 0.04040000 0.03190000 0.2555
## 3  3 0.02600000 0.03800000 0.02760000 0.2727
## 4  4 0.02719999 0.03929999 0.03590000 0.1978
## 5  5 0.04079999 0.06200000 0.06999999 0.2210
## 6  6 0.04020000 0.05670000 0.06260000 0.2209</code></pre>
<pre class="r"><code>vr_p &lt;- extract(v_r, points, drop=F)
head(vr_p)</code></pre>
<pre><code>##   ID          b          g          r       nir
## 1  1 0.02557389 0.05894550 0.03758506 0.3497387
## 2  2 0.01882389 0.05033908 0.02247731 0.3220550
## 3  3 0.01982662 0.05362828 0.02300457 0.3844531
## 4  4 0.02047731 0.05194858 0.02767093 0.3193594
## 5  5 0.04172961 0.09754695 0.07302261 0.4105712
## 6  6 0.03550068 0.07566584 0.05827252 0.2950630</code></pre>
<p>Create a model.</p>
<pre class="r"><code>data &lt;- data.frame(S2=s2_p[,-1], UAV=vr_p[,-1])
head(data)</code></pre>
<pre><code>##         S2.b       S2.g       S2.r S2.nir      UAV.b      UAV.g      UAV.r
## 1 0.03010000 0.04340000 0.04370000 0.2039 0.02557389 0.05894550 0.03758506
## 2 0.02669999 0.04040000 0.03190000 0.2555 0.01882389 0.05033908 0.02247731
## 3 0.02600000 0.03800000 0.02760000 0.2727 0.01982662 0.05362828 0.02300457
## 4 0.02719999 0.03929999 0.03590000 0.1978 0.02047731 0.05194858 0.02767093
## 5 0.04079999 0.06200000 0.06999999 0.2210 0.04172961 0.09754695 0.07302261
## 6 0.04020000 0.05670000 0.06260000 0.2209 0.03550068 0.07566584 0.05827252
##     UAV.nir
## 1 0.3497387
## 2 0.3220550
## 3 0.3844531
## 4 0.3193594
## 5 0.4105712
## 6 0.2950630</code></pre>
<pre class="r"><code># Plot the data
plot(S2.b~UAV.b, data=data, pch=16)
# Create a linear regression model
l.model &lt;- lm(S2.b~UAV.b, data=data)
# Add the fitted line
abline(l.model, col=&quot;red&quot;)</code></pre>
<p><img src="fusion_files/figure-html/m4-1.png" width="672" /></p>
<p>Looks like the relation within the blue band is not linear. Let’s try a non-linear SVM model.</p>
<pre class="r"><code>#SVM
library(e1071)
svm.model &lt;- svm(S2.b~UAV.b, data=data)
svm.pred &lt;- predict(svm.model, data)
#SVM
library(randomForest)
rfmod &lt;- randomForest(S2.b~UAV.b, data=data)
rf.pred &lt;- predict(rfmod, data)
x11()
plot(S2.b~UAV.b,data, pch=16)
points(data$S2.b, svm.pred, col = &quot;blue&quot;, pch=4)
points(data$S2.b, rf.pred, col = &quot;red&quot;, pch=4)
legend(&quot;topright&quot;,c(&quot;Data&quot;,&quot;SVM&quot;,&quot;RF&quot;), pch= c(16, 4, 4),col=c(&quot;black&quot;, &quot;blue&quot;,&quot;red&quot;))</code></pre>
<p><img src="fusion_files/figure-html/m5-1.png" width="672" /></p>
</div>
<div id="predicting-sentinel-2-reflectance" class="section level3">
<h3>Predicting Sentinel 2 reflectance</h3>
<p>SVM and RF models have better characterized the relationship between S2 and UAV blue bands. Let predict high resolution S2 band from existing UAV.</p>
<pre class="r"><code>UAV &lt;- v1[[&#39;b&#39;]] 
names(UAV) &lt;-&#39;UAV.b&#39;
s2h.b &lt;- predict(UAV, rfmod, na.rm=T)

s2h.svm &lt;- predict(UAV, svm.model, na.rm=T)

x11()
par(mfrow = c(1, 2), mar = c(4, 5, 1.4, 0.2)) 
plot(s2h.svm,  main=&quot;svm S2 1m Blue band&quot;)
plot(s2h.b, main=&quot;RF S2 1m Blue band&quot;)</code></pre>
<p><img src="fusion_files/figure-html/m6-1.png" width="672" /></p>
<pre class="r"><code># #parallel processing in raster
# UAv &lt;- raster(UAV)
# library(snow)
# startTime &lt;- Sys.time() 
# cat(&quot;Start time&quot;, format(startTime),&quot;\n&quot;)
# beginCluster()
#  r4 &lt;- predict(UAV, rfmod,na.rm=T)
# endCluster()
# timeDiff &lt;- Sys.time() - startTime
# cat(&quot;\n Processing time&quot;, format(timeDiff), &quot;\n&quot;)</code></pre>
</div>
<div id="validation" class="section level3">
<h3>Validation</h3>
<p>But how do we know if this predictions are accurate? We can plot predicted vs actual and also compute RMSE and Mean Absolute Percentage Error (MAPE) using training data. MAPE is given as:</p>
<pre class="r"><code>MAPE &lt;- function (y_pred, y_true){
    MAPE &lt;- mean(abs((y_true - y_pred)/y_true))
    return(MAPE*100)
}</code></pre>
<p>and RMSE,</p>
<pre class="r"><code>rmse &lt;- function(error){
  sqrt(mean(error^2))
}</code></pre>
<p>So lets compute MAPE and RMSE for both methods.</p>
<pre class="r"><code>svm.rmse &lt;- rmse(svm.pred-data$S2.b)
svm.rmse</code></pre>
<pre><code>## [1] 0.003996155</code></pre>
<pre class="r"><code>rf.rmse &lt;- rmse(rf.pred-data$S2.b)
rf.rmse</code></pre>
<pre><code>## [1] 0.002186289</code></pre>
<pre class="r"><code>svm.mape &lt;- MAPE(svm.pred, data$S2.b)
svm.mape</code></pre>
<pre><code>## [1] 7.315199</code></pre>
<pre class="r"><code>rf.mape &lt;- MAPE(rf.pred, data$S2.b)
rf.mape</code></pre>
<pre><code>## [1] 4.358044</code></pre>
<p>From the validations Random Forest gives better prediction than Support Vector machines because it has low MAPE and RMSE error. In that case we can adopt the high resolution S2 image predicted/simulated by RF and fuse it with UAV image at 1 m resolution. Note that we only predict the blue band, we can model and predict the other bands and then fuse them.</p>
<pre class="r"><code>#Green band
rfmod &lt;- randomForest(S2.g~UAV.g, data=data)
UAV=subset(v1,&#39;g&#39;)
names(UAV) &lt;-&#39;UAV.g&#39;
s2h.g &lt;- predict(UAV, rfmod, na.rm=T)
#red band
rfmod &lt;- randomForest(S2.r~UAV.r, data=data)
UAV=subset(v1,&#39;r&#39;)
names(UAV) &lt;-&#39;UAV.r&#39;
s2h.r &lt;- predict(UAV, rfmod, na.rm=T)
#NIR band
rfmod &lt;- randomForest(S2.nir~UAV.nir, data=data)
UAV=subset(v1,&#39;nir&#39;)
names(UAV) &lt;-&#39;UAV.nir&#39;
s2h.nir &lt;- predict(UAV, rfmod, na.rm=T)

#Stack them
s2h &lt;- stack(x=c(s2h.b,s2h.g,s2h.r,s2h.nir))
names(s2h) &lt;- c(&quot;b&quot;, &quot;g&quot;,&quot;r&quot;, &quot;nir&quot;)
s2h</code></pre>
<pre><code>## class      : RasterStack 
## dimensions : 1570, 760, 1193200, 4  (nrow, ncol, ncell, nlayers)
## resolution : 1, 1  (x, y)
## extent     : 390450, 391210, 7070170, 7071740  (xmin, xmax, ymin, ymax)
## crs        : NA 
## names      :          b,          g,          r,        nir 
## min values : 0.02305458, 0.03414621, 0.02679058, 0.12475280 
## max values : 0.06060897, 0.08070807, 0.10184987, 0.30137640</code></pre>
<p>However, the predicted reflectance ranges in all bands except NIR are very low. Why could this be so? Now lets create a high resolution fused image using Zhou’s approach.</p>
</div>
<div id="fusion" class="section level3">
<h3>Fusion</h3>
<pre class="r"><code>s2h.fused &lt;- (rast(s2h)/v1)*v1
s2h.fused</code></pre>
<pre><code>## class       : SpatRaster 
## dimensions  : 1570, 760, 4  (nrow, ncol, nlyr)
## resolution  : 1, 1  (x, y)
## extent      : 390450, 391210, 7070170, 7071740  (xmin, xmax, ymin, ymax)
## coord. ref. :  
## source      : memory 
## names       :          b,          g,          r,        nir 
## min values  : 0.02305458, 0.03414621, 0.02679058, 0.12475280 
## max values  : 0.06060897, 0.08070807, 0.10184987, 0.30137640</code></pre>
<pre class="r"><code>x11()
par(mfrow = c(1, 2), mar = c(4, 5, 1.4, 0.2)) 
plotRGB(s2h.fused, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;,  main=&quot;UAV+S2 high res&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(v, r=&quot;nir&quot;, g=&quot;r&quot;, b=&quot;g&quot;, stretch=&quot;lin&quot;, main=&quot;UAV&quot;, axes=T, mar = c(4, 5, 1.4, 0.2))
box()</code></pre>
<p><img src="fusion_files/figure-html/m9-1.png" width="672" /></p>
<p>The other option would be use S2 to predict UAV in areas not covered by the drone and then fuse it with S2. However this approach would require consideration of similar land-cover. For instance, we can not train the model in an area with different land-cover say cropland and predict in another area with say Forest. Food for thought.</p>
<p>But let’s step back a little, could sampling from land-cover categories have improved prediction of high resolution S2 done previously with RF? To test this, let us perform a K-means classification and sample from its land-cover map.</p>
<pre class="r"><code>image &lt;- v_r
image[is.na(image)] &lt;- 0
nclass &lt;- 4
system.time(
E &lt;- kmeans(as.data.frame(image, na.rm=F), nclass, iter.max = 100, nstart = 9)
)</code></pre>
<pre><code>##    user  system elapsed 
##    0.11    0.00    0.11</code></pre>
<pre class="r"><code>k.map &lt;- image[[1]]
values(k.map) &lt;- E$cluster
k.map[is.na(v_r[[1]])] &lt;- NA
plot(k.map)</code></pre>
<p><img src="fusion_files/figure-html/m10-1.png" width="672" /></p>
<p>Sample points using stratified random sampling based on the K-means land-cover map.</p>
<pre class="r"><code>set.seed(530)
points &lt;- spatSample(k.map, 900, &quot;stratified&quot;, as.points=T, na.rm=T, values=F)
x11()
plot(k.map,  main=&quot;K-means +sampling points&quot;, axes=T, mar = c(4, 5, 1.4, 0.2), col=topo.colors(max(values(k.map),na.rm=T)))
plot(points,add=T)</code></pre>
<p><img src="fusion_files/figure-html/m11-1.png" width="672" /></p>
<p>Use the stratified randomly sample points to train and predict simulated high resolution S2.</p>
<pre class="r"><code>s2_p &lt;- extract(s, points, drop=F)
head(s2_p)</code></pre>
<pre><code>##   ID          b          g          r    nir
## 1  1 0.04770000 0.05939999 0.07750000 0.1492
## 2  2 0.03960000 0.05549999 0.07099999 0.2073
## 3  3 0.03320000 0.04880000 0.05010000 0.2258
## 4  4 0.03379999 0.04790000 0.06040000 0.1966
## 5  5 0.04420000 0.06200000 0.07959999 0.2172
## 6  6 0.04070000 0.05980000 0.06649999 0.2101</code></pre>
<pre class="r"><code>vr_p &lt;- extract(v_r, points, drop=F)
head(vr_p)</code></pre>
<pre><code>##   ID          b          g          r       nir
## 1  1 0.05413008 0.09205989 0.11132049 0.2734993
## 2  2 0.04446261 0.10010008 0.08397383 0.4695161
## 3  3 0.03073439 0.07668109 0.05490024 0.4001549
## 4  4 0.02435945 0.06213900 0.04357978 0.3155672
## 5  5 0.03281799 0.06734130 0.06080524 0.2811245
## 6  6 0.03464006 0.07732372 0.05422943 0.3122228</code></pre>
<pre class="r"><code>data &lt;- data.frame(S2=s2_p[,-1], UAV=vr_p[,-1])
rfmod &lt;- randomForest(S2.b~UAV.b, data=data)
rf.pred &lt;- predict(rfmod, data)</code></pre>
<p>Lets evaluate if stratified random sampling improved RF accuracy.</p>
<pre class="r"><code>rf.rmse &lt;- rmse(rf.pred-data$S2.b)
rf.rmse</code></pre>
<pre><code>## [1] 0.00193131</code></pre>
<pre class="r"><code>rf.mape &lt;- MAPE(rf.pred, data$S2.b)
rf.mape</code></pre>
<pre><code>## [1] 4.36695</code></pre>
<p>We can see that RMSE and MAPE have reduced by small margins compared to the case of random sampling. Thus there is a chance that sampling over different land-cover improves prediction.</p>
<p>As an assignment now predict S2 high resolution using a RF model trained on stratified random samples from K-means classifier.</p>
</div>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Thierry Ranchin and Lucien Wald. <em>Data Fusion in Remote Sensing of Urban and Suburban Areas</em>, pages 193–218. Springer Netherlands, Dordrecht, 2010. ISBN 978-1-4020-4385- 7. doi: 10:1007/978-1-4020-4385-7 11. URL <a href="https://doi.org/10.1007/978-1-4020-4385-7_11" class="uri">https://doi.org/10.1007/978-1-4020-4385-7_11</a>.</p>
<p>Y. Zou, G. Li and S. Wang, “The Fusion of Satellite and Unmanned Aerial Vehicle (UAV) Imagery for Improving Classification Performance,” <em>IEEE International Conference on Information and Automation (ICIA)</em>, 2018, pp. 836-841, doi: 10.1109/ICInfA.2018.8812312.</p>
</div>

<p> Created 14th May 2021 Copyright &copy; Benson Kenduiywo, Inc. All rights reserved.</p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
