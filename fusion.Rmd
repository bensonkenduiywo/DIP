---
title: "Image Fusion"
output: 
  html_document:
    toc: true
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This tutorial seeks to illustrate how image fusion can be conducted. We use an Unmanned Aerial Vehicle (UAV) and Sentinel 2 optical image acquired within the same area and period. **Data fusion** is a formal framework in which are expressed means and tools for the alliance of data originating from different sources. It aims at obtaining information of greater quality; the exact definition of ”greater quality” will depend upon the application ([Ranchin and Wald, 2010](https://doi.org/10.1007/978-1-4020-4385-7_11)).

The principal motivation for image fusion is to improve the quality of the information
contained in the output image in a process known as synergy. A study of existing image
fusion techniques and applications shows that image fusion can provide us with an output
image with an improved quality. In this case, the benefits of image fusion include:

1. Extended range of operation.
2. Extended spatial and temporal coverage.
3. Reduced uncertainty.
4. Increased reliability.
5. Robust system performance.
6. Compact representation of information.

## Data preparation

Load libraries, declare variables and data paths.

```{r d1, message=FALSE}
rm(list=ls(all=TRUE))    #Clears R memory
unlink(".RData") 
if (!require("pacman")) install.packages("pacman"); library(pacman) #package manager( loads required packages/libraries in list as below if not installed they will be installed
p_load(raster, terra)

options(warn=1)
cat("Set variables and start processing\n")
Root 		<- 'D:/JKUAT/RESEARCH_Projects/Eswatini/Data/'
Path_out    <- paste0(Root,"Output/")

```

Let us load UAV and Sentinel 2 optical images that we will use. 

```{r d2}
path <- list.files(paste0(Root,'S2/interim/'),pattern = (".tif$"), recursive = TRUE, full.names = TRUE)
path
s <- rast(path)
s
path <- list.files(paste0(Root,'WingtraOne/'),pattern = (".tif$"), recursive = TRUE, full.names = TRUE)
#reorder the bands
paths <- path
paths[3] <- path[4]
paths[4] <- path[3]
paths
v <- rast(paths)
v
```

Now check image properties and assign meaningful names to its bands.

```{r d3}
#Resolution
res(s)
#Extents
ext(s)
#image dimensions
dim(s)
#Number of bands
nlyr(s)
names(s) <- c("b", "g","r", "nir")
s
res(v)
ext(v)
dim(v)
names(v) <- c("b", "g","r", "nir")
v
```

Crop Sentinel 2 image to UAV image extents.

```{r d4}
s <- crop(s, ext(v), snap="near")
```

Display the images side by side.

```{r d5, message = FALSE}
x11()
par(mfrow = c(1, 2)) #c(bottom, left, top, right)
plotRGB(s, r="nir", g="r", b="g", stretch="lin", axes=T, mar = c(4, 5, 1.4, 0.2), main="S2")
box()
plotRGB(v, r="nir", g="r", b="g", stretch="lin", axes=T, mar = c(4, 5, 1.4, 0.2), main="UAV")
box()

```

First let us conduct a spectral fusion of S2 and UAV images. To do this we have to resample UAV image to S2 extents.

```{r d6}
v_r <- resample(v, s, method='bilinear')
ext(v_r) == ext(s)
```

## Pixel fusion

This sections considers fusion techniques which rely on simple pixel based operations on the input image values. The assumption is that the input images are spatially and temporally aligned, semantically equivalent and radiometrically calibrated. Therefore, let us fuse the two images by multiplication and display it against the original ones.

```{r d7}
#Fuse by multiplication
f1 <- s * v_r
f1
#Display fused image alongside original UAV
x11()
par(mfrow = c(1, 3),mar = c(4, 5, 1.4, 0.2)) 
plotRGB(v, r="nir", g="r", b="g", stretch="lin", axes=T, mar = c(4, 5, 1.4, 0.2), main="UAV", cex.axis=0.7)
box()
plotRGB(s, r="nir", g="r", b="g", stretch="lin", axes=T, mar = c(4, 5, 1.4, 0.2), main="S2", cex.axis=0.7)
box()
plotRGB(f1, r="nir", g="r", b="g", stretch="lin", axes=T, mar = c(4, 5, 1.4, 0.2), main="Fused_multi", cex.axis=0.7)
box()
```

What about mean fusion (i.e. taking the mean of each pixel's reflectance in both UAV and S2)?

```{r d8}
#Fuse by multiplication
f2 <- mean(s, v_r)
f2
#Display fused image alongside original UAV
x11()
par(mfrow = c(2, 2), mar = c(4, 5, 1.4, 0.2)) 
plotRGB(v, r="nir", g="r", b="g", stretch="lin",  main="UAV", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(s, r="nir", g="r", b="g", stretch="lin", main="S2", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f1, r="nir", g="r", b="g", stretch="lin", main="Fused_mult", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f2, r="nir", g="r", b="g", stretch="lin", main="Fused_mean", axes=T, mar = c(4, 5, 1.4, 0.2))
box()

```

Let us finally follow the fusion approach in [Zou et al (2018)](https://ieeexplore.ieee.org/document/8812312).

```{r d9}
f3 = (s/v_r)*v_r
x11()
par(mfrow = c(2, 3), mar = c(4, 5, 1.4, 0.2)) 
plotRGB(v, r="nir", g="r", b="g", stretch="lin",  main="UAV", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(s, r="nir", g="r", b="g", stretch="lin", main="S2", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f1, r="nir", g="r", b="g", stretch="lin", main="Fused_mult", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f2, r="nir", g="r", b="g", stretch="lin", main="Fused_mean", axes=T, mar = c(4, 5, 1.4, 0.2))
box()
plotRGB(f3, r="nir", g="r", b="g", stretch="lin", main="Zhou etal", axes=T, mar = c(4, 5, 1.4, 0.2))
box()

```


## Feature based fusion

In *feature fusion* we fuse together the features $F_k,k \in{1,2, \dots, K}$. These features can be vegetation indices like Normalized Difference Index (NDVI) or feature maps  that have been made semantically equivalent by transforming them into probabilistic $p(m,n)$, or likelihood, maps.

Let us start with NDVI ($\text{NDVI}=\frac{\text{NIR}-\text{Red}}{\text{NIR}+\text{Red}}$. First compute NDVI for both UAV and S2.

```{r n1}
n_v <- (subset(v_r,"nir")-subset(v_r,"r"))/(subset(v_r,"nir")+subset(v_r,"r"))
n_s <- (subset(s,"nir")-subset(s,"r"))/(subset(s,"nir")+subset(s,"r"))
x11()
par(mfrow = c(1, 2), mar = c(5, 5, 1.4, 0.2))#c(bottom, left, top, right)
plot(n_v, main="UAV NDVI")
plot(n_s, main="S2 NDVI")
```

How can we fuse the NDVI index? Let us take an average of the two.

```{r n2}
nf <- mean(n_v, n_s)
x11()
plot(nf, main="Fused NDVI")
```

Is there a difference between UAV NDVI only and the fused one? Let's evaluate the images below.

```{r n3}
x11()
plot(n_v, main="UAV NDVI")
x11()
plot(nf, main="Fused NDVI")
```

## Modelling UAV reflectance

Due to the cost of UAV field operations can we model and predict its reflectance given? Let's check the relationship between the two.

```{r m1}
x11()
par(mfrow = c(2, 2), mar = c(4, 5, 1.4, 0.2)) 
plot(as.vector(subset(s,'b')),as.vector(subset(v_r,'b')), xlab='S2', ylab='W1 UAV', main="Blue band",pch=16,cex=0.75, col='blue')
plot(as.vector(subset(s,'g')),as.vector(subset(v_r,'g')), xlab='S2', ylab='W1 UAV', main="Green band",pch=16,cex=0.75, col='green')
plot(as.vector(subset(s,'r')),as.vector(subset(v_r,'r')), xlab='S2', ylab='W1 UAV', main="Red band",pch=16,cex=0.75, col='red')
plot(as.vector(subset(s,'nir')),as.vector(subset(v_r,'nir')), xlab='S2', ylab='W1 UAV', main="NIR band",pch=16,cex=0.75)
```

There seem to be some linear relationship between UAV and Sentinel 2 surface reflectance. However it is evident that reflectance values from UAV are higher compare to those in Sentinel 2. So what now?

## Spatial-spectral fusion

Previously we downsampled the UAV image in order to conduct fusion. While this reduces spectral variability it destroys spatial resolution. Therefore, in this section we will first upsample the Satellite image to match UAV spatial resolution and then proceed to condut image fusion. This way, we will improve both spatial and spectral information of Sentinel 2 image. 

## References

Thierry Ranchin and Lucien Wald. *Data Fusion in Remote Sensing of Urban and Suburban
Areas*, pages 193–218. Springer Netherlands, Dordrecht, 2010. ISBN 978-1-4020-4385-
7. doi: 10:1007/978-1-4020-4385-7 11. URL https://doi.org/10.1007/978-1-4020-4385-7_11.

Y. Zou, G. Li and S. Wang, "The Fusion of Satellite and Unmanned Aerial Vehicle (UAV) Imagery for Improving Classification Performance," *IEEE International Conference on Information and Automation (ICIA)*, 2018, pp. 836-841, doi: 10.1109/ICInfA.2018.8812312.
